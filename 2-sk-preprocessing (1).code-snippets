// =============================================================================================================================
//                                                  EDA Code Sections
// =============================================================================================================================
// Data Reading
// - Read CSV               -> read: sk-read-csv (With commonly used parameters)
// - Read Excel             -> read: sk-read-excel
// - Read Feather           -> read: sk-read-feather
// - Read Parquet           -> read: sk-read-parquet

// Basic EDA
// - Basic EDA Summary      -> eda: (1) Quick   (shape, columns, info, describe, missing, duplicates)
// - Optimize DataFrame     -> eda: optimize_df (memory optimization, dtypes conversion)
// - Large Variables        -> vars: manage     (show & delete large variables)
// - Drop Operations        -> eda: Drop        (drop columns or rows from DataFrame)

// Target & Distribution Analysis
// - Target Distribution    -> eda: (2) target-distribution         (classification/regression plots)
// - Column Types Summary   -> eda: (3) col-types                   (identify numerical & categorical)
// - Numeric Distributions  -> eda: (4.1) numeric-distribution      (countplot/KDE based on threshold)
// - Numeric Grid Plot      -> eda: (4.2) numeric_grid              (grid layout distributions)
// - Continuous by Target   -> eda: (4.3) one continuous to target  (histogram + boxplot)

// Data Quality & Missing Values
// - Missing Values Report  -> eda: (5) missing_df                  (count, percentage, colored by type)

// Correlation Analysis
// - Correlation Matrix     -> eda: (6) corr-matrix                 (heatmap with high correlation analysis)

// Categorical Analysis
// - Categorical Summary    -> eda: (7) cat-summary                 (top categories with plots)
// - Split by Cardinality   -> eda: (8) split_cat_card              (low/medium/high cardinality)

// Outlier Analysis
// - OutlierHandler Class   -> eda: (9.1) OutlierHandler            (OOP approach for outlier handling)
// - Outlier Detection      -> eda: (9.2) outliers detect           (IQR, Z-score, Isolation Forest, etc.)
// - Outlier Handling       -> eda: (9.3) outliers Handling         (remove, cap, transform methods)

// Feature Engineering
// - Binning Operations     -> eda: Binning                         (convert numerical to categorical bins)
// - Skewness Analysis      -> eda: skew                            (analyze and visualize skewness with transformations)
// ================================================================================================================================
{
"Read CSV": {
    "prefix": "read: sk-read-csv",
    "description": "Read CSV file into pandas DataFrame with key parameters",
    "body": [
        "# 🔹 Commonly used parameters in pd.read_csv:",
        "# sep=','              -> Column separator (',' default, use '\\t' for TSV)",
        "# header=0             -> Row of column names (None if no header)",
        "# names=[...]          -> Define column names manually",
        "# index_col=0          -> Column to use as index",
        "# usecols=[...]        -> Load only specific columns",
        "# nrows=1000           -> Read only first N rows",
        "# skiprows=5           -> Skip first 5 rows (or list of rows)",
        "# na_values=['?']      -> Extra strings to treat as NaN",
        "# dtype={'col': str}   -> Force data type for column",
        "# parse_dates=['col']  -> Parse column(s) as datetime",
        "# dayfirst=True        -> Dates in dd/mm/yyyy format",
        "# encoding='utf-8'     -> File encoding ('latin1', 'cp1256' for Arabic)",
        "# compression='zip'    -> If file is compressed",
        "# chunksize=10000      -> Read file in chunks (returns iterator)",
        "# on_bad_lines='skip'  -> Skip bad/invalid rows",
        "",
        "${1:df} = pd.read_csv('${2:data.csv}')",
        "$1.head()",
    ]
},
// =========================
"Read Excel": {
    "prefix": "read: sk-read-excel",
    "description": "Read Excel file into pandas DataFrame with basic info",
    "body": [
        "${1:df} = pd.read_excel('${2:data.xlsx}')",
        "print($1.info())",
    ]
},
// =========================
"Read Feather": {
    "prefix": "read: sk-read-feather",
    "description": "Read Feather file into pandas DataFrame with basic info",
    "body": [
        "${1:df} = pd.read_feather('${2:data.feather}')",
        "print($1.info())",
    ]
},
// =========================
"Read Parquet": {
    "prefix": "read: sk-read-parquet",
    "description": "Read Parquet file into pandas DataFrame with basic info",
    "body": [
        "${1:df} = pd.read_parquet('${2:data.parquet}')",
        "print($1.info())",
    ]
},
// =========================
"Basic EDA Summary": {
    "prefix": "eda: (1) Quick",
    "description": "Perform basic exploratory data analysis (EDA) on a DataFrame, including shape, columns, info, summary stats, unique values, missing values, and duplicates",
    "body": [
        "display(df.shape)",
        "display(df.columns.tolist())",
        "display(df.info(memory_usage='deep'))",
        "display(df.describe().T)",
        "display(df.describe(include='object').T)",
        "display(df.nunique().sort_values(ascending=False))",
        "display(df.isnull().sum().to_frame().rename(columns={0:'Total No. of Missing Values'}))",
        "display(df.duplicated().sum())",
        "",
        "df.drop_duplicates(inplace=True)",
        "import missingno as msno",
        "msno.matrix(df)",
        "plt.show()",
        "",
        "# -----------------------------------------------------------------------",
        "# -------------- Commonly used parameters in df.describe() --------------",
        "# -----------------------------------------------------------------------",
        "",
        "# percentiles=[.1, .9]  ->  Custom quantiles (default [.25, .5, .75])",
        "# include='all'         ->  Show all columns (numeric + object + category)",
        "# include=['object']    ->  Show only categorical/text columns",
        "# exclude=['number']    ->  Exclude numeric columns",
    ]
},
// =========================
"Optimize DataFrame Dtypes": {
    "prefix": "eda: optimize_df",
    "description": "Optimize DataFrame memory usage: numeric downcast, categorical, boolean conversion (no datetime auto-conversion).",
    "body": [
    "def optimize_dataframe(df, verbose=True):",
    "    \"\"\"",
    "    Optimize DataFrame memory usage by:",
    "    - Downcasting numeric types",
    "    - Converting high-frequency strings to categorical",
    "    - Mapping boolean-like columns",
    "    \"\"\"",
    "",
    "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2",
    "",
    "    # Auto-convert object/string columns",
    "    for col in df.select_dtypes(include=['object', 'string']).columns:",
    "        series = df[col]",
    "        # Try numeric conversion",
    "        try:",
    "            df[col] = pd.to_numeric(series)",
    "            continue",
    "        except:",
    "            pass",
    "        # Convert booleans if <=2 unique values",
    "        if series.dropna().nunique() <= 2:",
    "            unique_vals = series.dropna().unique()",
    "            if set(unique_vals).issubset({'True','False','true','false','1','0',1,0}):",
    "                mapping = {",
    "                    'True': True, 'False': False,",
    "                    'true': True, 'false': False,",
    "                    '1': True, '0': False,",
    "                    1: True, 0: False",
    "                }",
    "                df[col] = series.map(mapping)",
    "                continue",
    "        # Convert to category if low cardinality",
    "        if series.nunique() / len(series) < 0.5:",
    "            df[col] = series.astype('category')",
    "",
    "    # Downcast numeric types",
    "    for col in df.select_dtypes(include=['int']).columns:",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')",
    "    for col in df.select_dtypes(include=['float']).columns:",
    "        df[col] = pd.to_numeric(df[col], downcast='float')",
    "",
    "    final_memory = df.memory_usage(deep=True).sum() / 1024**2",
    "    if verbose:",
    "        print(",
    "            f'Memory usage: {initial_memory:.1f} MB → {final_memory:.1f} MB '",
    "            f'({(initial_memory - final_memory) / initial_memory * 100:.1f}% reduction)'",
    "        )",
    "    return df",
    "",
    "# Usage example",
    "df = optimize_dataframe(df)"
    ]
},
// =========================
"Show & Delete Large Variables": {
    "prefix": "vars: manage",
    "description": "Show variables larger than X MB and optionally bulk delete them",
    "body": [
        "def show_variables(scope=\"global\", min_size_mb=10):",
        "    \"\"\"Show variables in memory larger than a given size (MB) sorted from largest to smallest.\"\"\"",
        "    import sys",
        "    variables = globals() if scope == \"global\" else locals()",
        "    var_info  = []",
        "    excluded_vars = {'_oh', '_dh', '_', 'exit', 'quit', 'get_ipython', 'In', 'Out'}",
        "    ",
        "    for name, obj in variables.items():",
        "        if (name.startswith(\"__\") and name.endswith(\"__\")) or name.startswith(\"_i\") or (name.startswith(\"_\") and name[1:].isdigit()) or name in excluded_vars:",
        "            continue",
        "        try:",
        "            if isinstance(obj, pd.DataFrame):",
        "                size = obj.memory_usage(deep=True).sum() / 1024 ** 2",
        "            else:",
        "                size = sys.getsizeof(obj) / 1024 ** 2",
        "        except:",
        "            size = 0",
        "        if size >= min_size_mb:",
        "            var_info.append((name, type(obj).__name__, size))",
        "    ",
        "    var_info.sort(key=lambda x: x[2], reverse=True)",
        "    df_vars = pd.DataFrame(var_info, columns=[\"Name\", \"Type\", \"Size_MB\"])",
        "    df_vars[\"Size (MB)\"] = df_vars[\"Size_MB\"].apply(lambda x: f\"{x:.2f} MB\")",
        "    return df_vars.drop(\"Size_MB\", axis=1).reset_index(drop=True)",
        "",
        "############################################################################",
        "def bulk_delete(vars_list, scope=\"global\", save_deleted=False, return_type=\"dict\"):",
        "    \"\"\"Delete multiple variables from memory (global or local scope).\"\"\"",
        "    deleted_vars = {} if return_type == \"dict\" else []",
        "    variables = globals() if scope == \"global\" else locals()",
        "    for var_name in vars_list:",
        "        if var_name in variables:",
        "            if save_deleted:",
        "                if return_type == \"dict\":",
        "                    deleted_vars[var_name] = variables[var_name]",
        "                else:",
        "                    deleted_vars.append(variables[var_name])",
        "            del variables[var_name]",
        "    gc.collect()",
        "    return deleted_vars if save_deleted else None",
        "",
        "############################################################################",
        "def show_var(MB=1):",
        "    display(show_variables(min_size_mb=MB))",
        "",
        "############################################################################",
        "def delete(vars_list):",
        "    bulk_delete(vars_list)",
        "    display(show_variables(min_size_mb=1))",
        "",
        "############################################################################",
        "show_var()",
        "# delete(['df_pandas'])",
        "$0"
    ]
},
// =========================
"Drop columns or rows from DataFrame": {
    "prefix": "eda: Drop",
    "description": "Drop columns or rows from DataFrame",
    "body": [
        "# 🔹 Commonly used parameters in df.drop():",
        "# labels=['col']   ->  Column(s) or row(s) to drop",
        "# axis=1           ->  Drop columns (axis=0 for rows)",
        "# inplace=True     ->  Apply change directly to df",
        "# errors='ignore'  ->  Avoid error if label not found",
        "",
        "${1:df}.drop(labels=['${2:col_name}'], axis=1, inplace=True)"
    ]
},
// =========================
"Target Distribution Plot": {
    "prefix": "eda: (2) target-distribution",
    "description": "Plot target distribution for classification or regression with optional log scale for classification",
    "body": [
        "def target_distribution(df, target, task, title='Target Distribution', color=['skyblue', 'salmon'], Log=None):",
        "    if task == 'classification':",
        "        counts = df[target].value_counts()",
        "        balance_ratio = counts.max() / counts.min() if counts.min() > 0 else float('inf')",
        "        print(f\"Balance Ratio: {balance_ratio:.2f} (1.0 = perfectly balanced)\")",
        "        fig, axs = plt.subplots(1, 2, figsize=(15, 6))",
        "",
        "        # Barplot on left subplot",
        "        ax = axs[0]",
        "        sns.barplot(x=counts.index, y=counts.values, palette=color, ax=ax)",
        "        for p in ax.patches:",
        "            ax.annotate(f\"{int(p.get_height()):,}\",",
        "                        (p.get_x() + p.get_width() / 2., p.get_height()),",
        "                        ha='center', va='bottom', fontsize=10, color='black', xytext=(0, 3), textcoords='offset points')",
        "        ax.set_title(title, fontsize=14, weight='bold')",
        "        ax.set_xlabel(target)",
        "        ax.set_ylabel('Count')",
        "        if Log:",
        "            ax.set_yscale('log')",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)",
        "",
        "        # Pie chart on right subplot",
        "        ax2 = axs[1]",
        "        ax2.pie(counts.values, labels=counts.index, autopct='%1.1f%%', colors=color,",
        "                textprops={'fontweight': 'bold'}, explode=[0]*len(counts))",
        "        ax2.set_title(f\"{title} - Percentage\", fontsize=14, weight='bold')",
        "",
        "        plt.tight_layout()",
        "        plt.show()",
        "",
        "    elif task == 'regression':",
        "        sns.set_theme()",
        "        sns.histplot(df[target], kde=True, bins=30)",
        "        plt.title(title)",
        "        plt.xlabel(target)",
        "        plt.ylabel('Count')",
        "        plt.show()",
        "",
        "target_distribution(df, '${1:Exited}', task='${2:classification}', Log=${3:None})"
    ]
},
// =========================
"Get Column Types Summary": {
    "prefix": "eda: (3) col-types",
    "description": "Identify numerical and categorical columns in DataFrame with summary",
    "body": [
    "def get_column_types(df, numerics=[np.number], show_summary=True):",
    "    \"\"\"Identify numerical and categorical columns in a DataFrame.\"\"\"",
    "    from itertools import zip_longest",
    "    ",
    "    # Numeric columns",
    "    numerical_columns = df.select_dtypes(include=numerics).columns.tolist()",
    "    ",
    "    # Categorical columns",
    "    categorical_columns = df.select_dtypes(exclude=numerics).columns.tolist()",
    "    ",
    "    if show_summary:",
    "        print(f\"Column Type Summary:\")",
    "        print(f\"   Numerical   : {len(numerical_columns)} columns\")",
    "        print(f\"   Categorical : {len(categorical_columns)} columns\")",
    "        print(f\"   Total       : {len(df.columns)} columns\")",
    "        ",
    "        print(\"\\n Numerical Columns                                     | Categorical Columns\")",
    "        print(\"==========================================================================================\")",
    "        for i, (num, cat) in enumerate(zip_longest(numerical_columns, categorical_columns, fillvalue='')):",
    "            print(f\"{i+1:2d}. {num:<50} | {cat:<28}\")",
    "    ",
    "    return numerical_columns, categorical_columns",
    "",
    "############################################################################",
    "numerical_cols, categorical_cols = get_column_types(df)",
    ] 
},
// =========================
"Plot Numeric Distributions (1)": {
    "prefix": "eda: (4.1) numeric-distribution",
    "description": "Plot distributions of numeric features (countplot for discrete, KDE for continuous)",
    "body": [
    "def plot_numeric_distributions(df, numeric_features, threshold=20, cols_per_row=3):",
    "    # Calculate required rows",
    "    n_rows = (len(numeric_features) + cols_per_row - 1) // cols_per_row",
    "    plt.figure(figsize=(15, 5 * n_rows))",
    "    ",
    "    for i, col in enumerate(numeric_features):",
    "        plt.subplot(n_rows, cols_per_row, i + 1)",
    "        unique_vals = df[col].nunique()",
    "        ",
    "        # If column has <= threshold unique values → Discrete",
    "        if unique_vals <= threshold:",
    "            sns.countplot(x=col, data=df)",
    "            plt.title(f'{col} Countplot (Discrete)')",
    "        else:",
    "            # Otherwise consider it Continuous",
    "            sns.kdeplot(df[col], fill=True)",
    "            plt.title(f'{col} KDE Plot (Continuous)')",
    "    ",
    "    plt.tight_layout()",
    "    plt.show()",
    "    plt.close()",
    "",
    "plot_numeric_distributions(df, numeric_features)",
    ]
},
// =========================
"Plot Numeric Distributions (2)": {
    "prefix": "eda: (4.2) numeric_grid",
    "description": "Plot distributions of numeric features in a grid layout",
    "body": [
        "def plot_numeric_distributions_grid(df, numeric_features, cols_per_row=3, bins=30, rotation=0):",
        "    # Calculate required rows",
        "    n_rows = (len(numeric_features) + cols_per_row - 1) // cols_per_row",
        "    plt.figure(figsize=(5 * cols_per_row, 4 * n_rows))",
        "    ",
        "    for i, col in enumerate(numeric_features):",
        "        plt.subplot(n_rows, cols_per_row, i + 1)",
        "        sns.histplot(df[col], kde=True, bins=bins)",
        "        plt.title(f'{col} Distribution')",
        "        plt.xticks(rotation=rotation)",
        "    ",
        "    plt.tight_layout()",
        "    plt.show()",
        "",
        "plot_numeric_distributions_grid(df, numeric_features, cols_per_row=4, bins=50, rotation=45)",

    ]
},
// =========================
"plot continuous by target": {
    "prefix": "eda: (4.3) one continuous to target",
    "description": "Plot histogram and boxplot side-by-side for continuous feature by target",
    "body": [
    "def plot_continuous_by_target(df, column, target, bins=30, palette=\"Set2\", figsize=(13,6)):",
    "    plt.figure(figsize=figsize)",
    "    ",
    "    # Histogram with KDE",
    "    plt.subplot(1,2,1)",
    "    sns.histplot(data=df, x=column, hue=target, kde=True, bins=bins, palette=palette)",
    "    plt.title(f\"Distribution of {column} by {target}\", fontweight=\"bold\", pad=15, fontsize=14)",
    "    ",
    "    # Boxplot",
    "    plt.subplot(1,2,2)",
    "    sns.boxplot(data=df, y=column, x=target, palette=palette)",
    "    plt.title(f\"Boxplot of {column} by {target}\", fontweight=\"bold\", pad=15, fontsize=14)",
    "    ",
    "    plt.tight_layout()",
    "    plt.show()",
    "",
    "plot_continuous_by_target(df, column=\"Age\", target=\"Exited\")"
    ]
},
// =========================
"missing values count, percentage": {
    "prefix": "eda: (5) missing_df",
    "description": "Show data quality with missing values count, percentage, and colored type column",
    "body": [
    "def data_quality(df):",
    "    from pandas.api.types import is_numeric_dtype, is_object_dtype, CategoricalDtype",
    "    missing = df.isnull().sum()",
    "    missing_per = (missing / len(df)) * 100",
    "    missing_df = pd.DataFrame({",
    "        \"Missing_Count\"     : missing,",
    "        \"Missing_Percentage\": missing_per,",
    "        \"Column_Type\"       : [\"Numeric\" if is_numeric_dtype(df[c]) else \"Categorical\" if (is_object_dtype(df[c]) or isinstance(df[c].dtype, CategoricalDtype)) else \"Other\" for c in df.columns]",
    "    }).query(\"Missing_Count > 0\").sort_values(\"Missing_Percentage\", ascending=False)",
    "    if not missing_df.empty:",
    "        def color_column_type(val):",
    "            if val == \"Numeric\":",
    "                return \"background-color:#D6EAF8; color:#1B4F72; font-weight:bold;\"",
    "            elif val == \"Categorical\":",
    "                return \"background-color:#FADBD8; color:#641E16; font-weight:bold;\"",
    "            elif val == \"Datetime\":",
    "                return \"background-color:#E8F6F3; color:#0B5345; font-weight:bold;\"",
    "            else:",
    "                return \"background-color:#FDF2E9; color:#7D6608; font-weight:bold;\"",
    "        styled_df = (missing_df.style.applymap(color_column_type, subset=[\"Column_Type\"])",
    "                            .background_gradient(subset=[\"Missing_Percentage\"], cmap=\"Oranges\")",
    "                            .format({\"Missing_Count\": \"{:.0f}\", \"Missing_Percentage\": \"{:.2f}%\"}))",
    "        display(styled_df)",
    "    else:",
    "        print(\"No missing values detected.\")",
    "    return missing_df",
    "",
    "missing_df = data_quality(df)"
    ]
},
// =========================
"correlation matrix": {
    "prefix": "eda: (6) corr-matrix",
    "description": "Plot correlation matrix heatmap with optional column removal and high correlation analysis",
    "body": [
    "def plot_corr_matrix(df, continuous_columns=None, target: str = None, method: str = \"pearson\",",
    "                    figsize: tuple = (15, 8), cmap: str = \"coolwarm\", linewidths: float = 0.5,",
    "                    square: bool = True, fmt: str = \".2f\", mask: bool = False, annot: bool = True,",
    "                    title: str = None, threshold: float = 0.8, remove_target: str = None):",
    "    \"\"\"",
    "    Extended to also print high correlations with target and between features.",
    "    Optionally remove a column (e.g., target) from correlation matrix.",
    "    \"\"\"",
    "    if continuous_columns is None:",
    "        continuous_columns = df.select_dtypes(include=[np.number]).columns.tolist()",
    "    cols = continuous_columns.copy()",
    "    if remove_target and remove_target in cols:",
    "        cols.remove(remove_target)",
    "    corr_matrix = df[cols].corr(method=method)",
    "    mask_matrix = np.triu(np.ones_like(corr_matrix, dtype=bool)) if mask else None",
    "    if title is None:",
    "        title = f\"Correlation Matrix ({method})\"",
    "    plt.figure(figsize=figsize)",
    "    sns.heatmap(corr_matrix, annot=annot, fmt=fmt, cmap=cmap, linewidths=linewidths, square=square, mask=mask_matrix, cbar_kws={\"shrink\": 0.8})",
    "    plt.grid(False)",
    "    plt.title(title, fontsize=14)",
    "    plt.tight_layout()",
    "    plt.show()",
    "    ",
    "    results = {}",
    "    print(\"=\"*60)",
    "    print(f\" High Correlations (threshold ≥ {threshold}) \")",
    "    print(\"=\"*60)",
    "    if target is not None and target in corr_matrix.columns:",
    "        target_corr = corr_matrix[target].drop(target).dropna()",
    "        strong_target_corr = target_corr[abs(target_corr) >= threshold]",
    "        if not strong_target_corr.empty:",
    "            print(\"\\n🔹 Feature ←→ Target:\")",
    "            for feat, val in strong_target_corr.items():",
    "                print(f\"{feat}  ←→ {target} : {val:.3f}\")",
    "        results[\"target_corr\"] = strong_target_corr",
    "    else:",
    "        results[\"target_corr\"] = None",
    "    high_corr_pairs = []",
    "    for i in range(len(corr_matrix.columns)):",
    "        for j in range(i+1, len(corr_matrix.columns)):",
    "            col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]",
    "            corr_val = corr_matrix.iloc[i, j]",
    "            if target and (col1 == target or col2 == target):",
    "                continue",
    "            if abs(corr_val) >= threshold:",
    "                high_corr_pairs.append((col1, col2, corr_val))",
    "    if high_corr_pairs:",
    "        print(\"\\n🔹 Feature ←→ Feature:\")",
    "        for col1, col2, val in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):",
    "            print(f\"{col1} ←→ {col2} : {val:.3f}\")",
    "    results[\"high_corr_pairs\"] = high_corr_pairs",
    "    results[\"corr_matrix\"] = corr_matrix",
    "    return results",
    "",
    "results = plot_corr_matrix(df, continuous_columns=numeric_features, target=\"Exited\", method=\"spearman\", threshold=0.3, remove_target=\"Exited\")"
    ]
},
// =========================
"Categorical Summary & Plot": {
    "prefix": "eda: (7) cat-summary",
    "description": "Print top categories with 'others', percentages, and plot countplots",
    "body": [
        "def categorical_summary_plot(df, categorical_features, top_n=20, figsize=(8,5), palette='Set2'):",
        "    for col in categorical_features:",
        "        n_unique = df[col].nunique()",
        "        print(f\"{col:<15}   ➤ (Unique values: {n_unique}) Top {top_n} Categories with 'others' combined\")",
        "        value_counts = df[col].value_counts(normalize=True)",
        "        top_categories = value_counts.iloc[:top_n]",
        "        others_sum = value_counts.iloc[top_n:].sum()",
        "        combined = pd.concat([top_categories, pd.Series({'others': others_sum})])",
        "        for category, proportion in combined.items():",
        "            print(f\"  {category:<15} : {proportion*100:.2f}%\")",
        "        plt.figure(figsize=figsize)",
        "        plot_series = df[col].apply(lambda x: x if x in top_categories.index else 'others')",
        "        ax = sns.countplot(x=plot_series, palette=palette, order=combined.index)",
        "        total_count = len(df)",
        "        for p in ax.patches:",
        "            height = p.get_height()",
        "            ax.annotate(f'{height/total_count*100:.2f}%', (p.get_x() + p.get_width() / 2., height), ha='center', va='bottom', fontsize=10)",
        "        plt.title(f'Distribution of {col} (Top {top_n} + others)')",
        "        plt.xticks(rotation=45)",
        "        plt.tight_layout()",
        "        plt.show()",
        "        print('-'*50)",
        "",
        "categorical_summary_plot(df, categorical_features, top_n=${2:20})",
    ]
},
// =========================
"Split Categorical by Cardinality": {
    "prefix": "eda: (8) split_cat_card",
    "description": "Split categorical columns into low, medium, high cardinality and detect numeric columns",
    "body": [ 
        "def split_categorical_by_cardinality(df, categorical_cols=None, low_max=10, med_max=500):",
        "    if categorical_cols is None:",
        "        categorical_cols = df.select_dtypes(include=['object','category','bool']).columns.tolist()",
        "    ",
        "    # Compute cardinalities",
        "    cardinalities = {col: df[col].nunique() for col in categorical_cols}",
        "    ",
        "    low_card  = [c for c in categorical_cols if cardinalities[c] <= low_max]",
        "    med_card  = [c for c in categorical_cols if low_max < cardinalities[c] <= med_max]",
        "    high_card = [c for c in categorical_cols if cardinalities[c] > med_max]",
        "    ",
        "    numerical = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]",
        "    ",
        "    # Debug print",
        "    print(f\"Low    cardinality columns ({len(low_card):<3}): {low_card}\")",
        "    print(f\"Medium cardinality columns ({len(med_card):<3}): {med_card}\")",
        "    print(f\"High   cardinality columns ({len(high_card):<3}): {high_card}\\n\")",
        "    ",
        "    return low_card, med_card, high_card, numerical, cardinalities",
        "",
        "low_card, med_card, high_card, numerical_cols, card_dict = split_categorical_by_cardinality(df, low_max=5, med_max=100)",
    ]
},
// =========================
"OutlierHandler OOP": {
    "prefix": "eda: (9.1) OutlierHandler",
    "description": "Class for outlier detection, visualization, and handling (capping or interpolation) using IQR",
    "body": [
    "class OutlierHandler:",
    "    \"\"\"A class to handle outlier detection, visualization, and handling using IQR method.\"\"\"",
    "    def __init__(self, df, numerical_cols):",
    "        self.df = df.copy()",
    "        self.numerical_cols = numerical_cols",
    "        print(f\"Initialized OutlierHandler with {len(self.numerical_cols)} numerical columns.\")",
    "",
    "    def detect_outliers_iqr(self, feature):",
    "        Q1 = self.df[feature].quantile(0.25)",
    "        Q3 = self.df[feature].quantile(0.75)",
    "        IQR = Q3 - Q1",
    "        lower_bound = Q1 - 1.5 * IQR",
    "        upper_bound = Q3 + 1.5 * IQR",
    "        return self.df[(self.df[feature] < lower_bound) | (self.df[feature] > upper_bound)]",
    "",
    "    def plot_outliers(self, n_cols=6, color='skyblue'):",
    "        import math",
    "        n_rows = math.ceil(len(self.numerical_cols) / n_cols)",
    "        plt.figure(figsize=(5 * n_cols, 4 * n_rows))",
    "        for i, col in enumerate(self.numerical_cols, 1):",
    "            plt.subplot(n_rows, n_cols, i)",
    "            sns.boxplot(y=self.df[col], color=color)",
    "            plt.title(f'Box Plot - {col}')",
    "        plt.tight_layout()",
    "        plt.show()",
    "",
    "    def cap_outliers_iqr(self):",
    "        for feature in self.numerical_cols:",
    "            Q1 = self.df[feature].quantile(0.25)",
    "            Q3 = self.df[feature].quantile(0.75)",
    "            IQR = Q3 - Q1",
    "            lower_bound = Q1 - 1.5 * IQR",
    "            upper_bound = Q3 + 1.5 * IQR",
    "            self.df[feature] = np.where(",
    "                self.df[feature] < lower_bound, lower_bound,",
    "                np.where(self.df[feature] > upper_bound, upper_bound, self.df[feature])",
    "            )",
    "        return self.df",
    "",
    "    def interpolate_outliers(self, method='linear'):",
    "        for feature in self.numerical_cols:",
    "            Q1 = self.df[feature].quantile(0.25)",
    "            Q3 = self.df[feature].quantile(0.75)",
    "            IQR = Q3 - Q1",
    "            lower_bound = Q1 - 1.5 * IQR",
    "            upper_bound = Q3 + 1.5 * IQR",
    "",
    "            # Replace outliers with NaN",
    "            self.df[feature] = self.df[feature].apply(",
    "                lambda x: np.nan if x < lower_bound or x > upper_bound else x",
    "            )",
    "            # Fill outliers using interpolation",
    "            self.df[feature] = self.df[feature].interpolate(method=method, limit_direction='both')",
    "        return self.df",
    "",
    "    def summary_outliers(self):",
    "        \"\"\"Print number of outliers in each numerical column.\"\"\"",
    "        for col in self.numerical_cols:",
    "            outliers = self.detect_outliers_iqr(col)",
    "            print(f\"{col:<17}: {len(outliers):<5} outliers\")",
    "",
    "# ================= Usage Example =================",
    "handler = OutlierHandler(df, numeric_features[:-1])  # exclude target if needed",
    "handler.plot_outliers(n_cols=4)",
    "handler.summary_outliers()",
    "",
    "# --- Option 1: Cap outliers ---",
    "df_capped = handler.cap_outliers_iqr()",
    "handler.summary_outliers()",
    "",
    "# --- Option 2: Interpolate outliers ---",
    "df_interpolated = handler.interpolate_outliers(method='linear')  # or 'spline', 'polynomial'",
    "handler.summary_outliers()"
    ]
},
// =========================
"Outlier Detection Methods": {
    "prefix": "eda: (9.2) outliers detect",
    "description": "Detect outliers using IQR, Z-score, Modified Z-score, Isolation Forest, DBSCAN",
    "body": [
        "### 1️⃣ IQR Rule",
        "Q1 = df['feature'].quantile(0.25)",
        "Q3 = df['feature'].quantile(0.75)",
        "IQR = Q3 - Q1",
        "outliers_iqr = (df['feature'] < Q1 - 1.5*IQR) | (df['feature'] > Q3 + 1.5*IQR)",
        "",
        "### 2️⃣ Z-Score (assumes normal distribution)",
        "from scipy import stats",
        "z_scores = np.abs(stats.zscore(df['feature']))",
        "outliers_z = z_scores > 3",
        "",
        "### 3️⃣ Modified Z-Score (robust, uses median)",
        "median = df['feature'].median()",
        "mad = np.median(np.abs(df['feature'] - median))",
        "modified_z = 0.6745 * (df['feature'] - median) / mad",
        "outliers_mod_z = np.abs(modified_z) > 3.5",
        "",
        "### 4️⃣ Isolation Forest",
        "from sklearn.ensemble import IsolationForest",
        "iso_forest = IsolationForest(contamination=0.1, random_state=42)",
        "outliers_iso = iso_forest.fit_predict(df[['feature']]) == -1",
        "",
        "### 5️⃣ DBSCAN (unsupervised clustering)",
        "from sklearn.cluster import DBSCAN",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)",
        "clusters = dbscan.fit_predict(df[['feature']])",
        "outliers_dbscan = clusters == -1",
        "$0"
    ]
},
// =========================
"Outlier Handling Methods": {
    "prefix": "eda: (9.3) outliers Handling",
    "description": "Remove, cap/floor, or transform outliers in a DataFrame column",
    "body": [
        "### 1️⃣ Remove Outliers",
        "# Filter rows where outliers are False",
        "outliers = (df['feature'] < df['feature'].quantile(0.01)) | (df['feature'] > df['feature'].quantile(0.99))",
        "df_clean = df[~outliers]",
        "",
        "### 2️⃣ Cap/Floor (Winsorizing)",
        "df['feature_capped'] = df['feature'].clip(",
        "    lower=df['feature'].quantile(0.01),",
        "    upper=df['feature'].quantile(0.99)",
        ")",
        "",
        "### 3️⃣ Transformations",
        "# Log transform (handles zeros)",
        "df['feature_log'] = np.log1p(df['feature'])",
        "",
        "# Box-Cox transform (shift if zeros exist)",
        "from scipy.stats import boxcox",
        "df['feature_boxcox'], _ = boxcox(df['feature'] + 1)",
        "",
        "### 4️⃣ Robust Scaling",
        "from sklearn.preprocessing import RobustScaler",
        "scaler = RobustScaler()",
        "df['feature_scaled'] = scaler.fit_transform(df[['feature']])",
        "$0"
    ]
},
// =========================
"Binning": {
    "prefix": "eda: Binning",
    "description": "Class for binning numerical features into categorical bins",
    "body": [
    "old_col = df['feature']",
    "# Convert to categories",
    "df['feature'] = pd.cut(df['feature'], bins=5, labels=['Low', 'Med-Low', 'Med', 'Med-High', 'High'])",
    ]
},
// =========================
"skew analysis": {
    "prefix": "eda: skew",
    "description": "Analyze skewness of numeric features and optionally visualize log transformation effect.",
    "body": [
        "# Calculate skewness for numeric features",
        "skew_df = df[numeric_features].skew().to_frame(name='Feature Skewness')",
        "skew_df['Feature Skewness'] = skew_df['Feature Skewness'].round(3)",
        "",
        "# Highlight highly skewed features (|skew| > 1)",
        "def highlight_skew(s):",
        "    is_skewed = s.abs() > 1",
        "    return ['background-color: red' if v else '' for v in is_skewed]",
        "",
        "styled_skew_df = skew_df.style.apply(highlight_skew, subset=['Feature Skewness'])",
        "styled_skew_df",
        "",
        "# Optional: visualize log transformation effect for a feature",
        "# old_val = df['feature']",
        "# df['feature'] = np.log1p(df['feature'])  # log(1+x) to handle zeros",
        "# new_val = df['feature']",
        "#",
        "# import matplotlib.pyplot as plt",
        "# import seaborn as sns",
        "# plt.figure(figsize=(13,6))",
        "# plt.subplot(1,2,1)",
        "# sns.histplot(old_val, color='purple', kde=True)",
        "# plt.title('Feature Distribution Before Transformation', fontweight='black', size=18, pad=20)",
        "#",
        "# plt.subplot(1,2,2)",
        "# sns.histplot(new_val, color='purple', kde=True)",
        "# plt.title('Feature Distribution After Transformation', fontweight='black', size=18, pad=20)",
        "# plt.tight_layout()",
        "# plt.show()"
    ]
}
}
