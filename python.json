{
"Data Generation and Datasets": {
    "prefix": "imports_(0)_datasets",
    "body": [
        "from sklearn.datasets import load_iris, load_breast_cancer, load_digits, make_classification, make_moons, make_circles, fetch_california_housing",
        "from sklearn import datasets"
    ],
    "description": "Imports for loading sample datasets and generating synthetic data for testing."
},
// ================================================================================================================================
"Basic Imports": {
    "prefix": "import_basic ",
    "body": [
        "import numpy as np",
        "import pandas as pd",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "import plotly.express as px",
        "import plotly.graph_objects as go",
        "import missingno as msno",
        "from matplotlib import cm",
        "from IPython.display import clear_output"
    ],
    "description": "Core imports for data manipulation, visualization, and styling"
},
// ================================================================================================================================
"Data Preprocessing": {
    "prefix": "imports_(1)_preprocessing",
    "body": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, PolynomialFeatures",
        "from sklearn.impute import SimpleImputer",
        "from sklearn.compose import ColumnTransformer, make_column_transformer",
        "from category_encoders import CountEncoder, TargetEncoder, BinaryEncoder",
        "from pandas.api.types import is_numeric_dtype, is_object_dtype, CategoricalDtype"
    ],
    "description": "Imports for data preprocessing, including encoding, scaling, imputation, and column transformation."
},
// ================================================================================================================================
"Data Splitting and Cross-Validation": {
    "prefix": "imports_(2)_splitting",
    "body": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, cross_val_score, cross_validate, validation_curve"
    ],
    "description": "Imports for splitting datasets and performing cross-validation or hyperparameter tuning."
},
// ================================================================================================================================
"Regression Models": {
    "prefix": "imports_(3)_regression",
    "body": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet",
        "from sklearn.neighbors import KNeighborsRegressor",
        "from sklearn.tree import DecisionTreeRegressor",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor",
        "from xgboost import XGBRegressor"
    ],
    "description": "Imports for regression algorithms, including linear, tree-based, and ensemble models."
},
// ================================================================================================================================
"Classification Models": {
    "prefix": "imports_(4)_classification",
    "body": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier",
        "from sklearn.neighbors import KNeighborsClassifier",
        "from sklearn.tree import DecisionTreeClassifier",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, VotingClassifier, StackingClassifier",
        "from sklearn.svm import SVC",
        "from xgboost import XGBClassifier",
        "from sklearn.multioutput import MultiOutputClassifier, OneVsRestClassifier",
        "from imblearn.ensemble import EasyEnsembleClassifier"
    ],
    "description": "Imports for classification algorithms, including linear, tree-based, ensemble, and specialized models."
},
// ================================================================================================================================
"Clustering and Dimensionality Reduction": {
    "prefix": "imports_(5)_clustering",
    "body": [
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering",
        "from sklearn.mixture import GaussianMixture",
        "from sklearn.decomposition import PCA",
        "from scipy.cluster.hierarchy import dendrogram, linkage",
        "from sklearn.ensemble import IsolationForest"
    ],
    "description": "Imports for clustering algorithms and dimensionality reduction techniques."
},
// ================================================================================================================================
"Handling Imbalanced Data": {
    "prefix": "imports_(6)_imbalanced",
    "body": [
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss",
        "from imblearn.combine import SMOTETomek, SMOTEENN",
        "from collections import Counter"
    ],
    "description": "Imports for handling imbalanced datasets with oversampling, undersampling, and hybrid techniques."
},
// ================================================================================================================================
"Model Evaluation Metrics": {
    "prefix": "imports_(7)_metrics",
    "body": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, classification_report, confusion_matrix, ConfusionMatrixDisplay, r2_score, mean_squared_error, mean_absolute_error",
        "from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances, cosine_distances",
        "from sklearn.metrics import silhouette_score, mutual_info_score"
    ],
    "description": "Imports for evaluating model performance, including classification, regression, and clustering metrics."
},
// ================================================================================================================================
"Statistical and Mathematical Utilities": {
    "prefix": "imports_(8)_stats",
    "body": [
        "from scipy import stats",
        "from scipy.stats import boxcox, skew, pearsonr, spearmanr",
        "from sklearn.feature_selection import SelectKBest, f_classif"
    ],
    "description": "Imports for statistical analysis and feature selection."
},
// ================================================================================================================================
"Miscellaneous Utilities": {
    "prefix": "imports_(9)_utils",
    "body": [
        "import gc",
        "import time",
        "import sys",
        "import json",
        "import yaml",
        "import requests",
        "from itertools import zip_longest",
        "from sklearn.utils import resample",
        "from sklearn.base import BaseEstimator, TransformerMixin",
        "from tqdm.auto import tqdm"
    ],
    "description": "Utility imports for memory management, warnings, timing, and custom transformers."
},
// ================================================================================================================================
"warnings": {
    "prefix": "import_warnings",
    "body": [
        "import warnings",
        "warnings.filterwarnings('ignore')"
            ],
    "description": "Import warnings module and set to ignore warnings."
},
// ================================================================================================================================
"optimize_dtypes": {
    "prefix": "optimize_dtypes",
    "body": [
        "def optimize_dtypes(df):",
        "    \"\"\"Optimize DataFrame memory usage by downcasting numeric types and converting strings to categories.\"\"\"",
        "    initial_memory = df.memory_usage(deep=True).sum() / 1024**2",
        "    ",
        "    # Downcast integers",
        "    for col in df.select_dtypes(include=['int']).columns:",
        "        df[col] = pd.to_numeric(df[col], downcast='integer')",
        "    ",
        "    # Downcast floats",
        "    for col in df.select_dtypes(include=['float']).columns:",
        "        df[col] = pd.to_numeric(df[col], downcast='float')",
        "    ",
        "    # Convert high-frequency strings to categorical",
        "    for col in df.select_dtypes(include=['string', 'object']).columns:",
        "        if df[col].nunique() < df.shape[0] * 0.5:  # Less than 50% unique",
        "            df[col] = df[col].astype('category')",
        "    ",
        "    final_memory = df.memory_usage(deep=True).sum() / 1024**2",
        "    print(f\"Memory optimization: {initial_memory:.1f}MB â†’ {final_memory:.1f}MB \"f\"({((initial_memory - final_memory) / initial_memory * 100):.1f}% reduction)\")",
        "    return df",
        "",
        "df = optimize_dtypes(df)"
    ],
    "description": "Function to optimize Pandas DataFrame memory usage by downcasting integers and floats, and converting low-cardinality string/object columns to categorical types."
},
// ================================================================================================================================
"Duplicates": {
    "prefix": "duplicates",
    "body": [
        "print(f'Duplicates: {df.duplicated().sum():,}')",
        "df = df.drop_duplicates().reset_index(drop=True)"
            ],
    "description": "Check and print the number of duplicate rows in the DataFrame."
},
// ================================================================================================================================
"show_variables": {
    "prefix": "show_variables",
    "body": [
        "def show_variables(scope=\"global\", min_size_mb=10):",
        "    \"\"\"Show variables in memory larger than a given size (MB) sorted from largest to smallest.\"\"\"",
        "    import sys",
        "    variables = globals() if scope == \"global\" else locals()",
        "    var_info = []",
        "    excluded_vars = {'_oh', '_dh', '_', 'exit', 'quit', 'get_ipython', 'In', 'Out'}",
        "    ",
        "    for name, obj in variables.items():",
        "        if (name.startswith(\"__\") and name.endswith(\"__\")) or name.startswith(\"_i\") or (name.startswith(\"_\") and name[1:].isdigit()) or name in excluded_vars:",
        "            continue",
        "        try:",
        "            if isinstance(obj, pd.DataFrame):",
        "                size = obj.memory_usage(deep=True).sum() / 1024 ** 2",
        "            else:",
        "                size = sys.getsizeof(obj) / 1024 ** 2",
        "        except:",
        "            size = 0",
        "        if size >= min_size_mb:",
        "            var_info.append((name, type(obj).__name__, size))",
        "    ",
        "    var_info.sort(key=lambda x: x[2], reverse=True)",
        "    df_vars = pd.DataFrame(var_info, columns=[\"Name\", \"Type\", \"Size_MB\"])",
        "    df_vars[\"Size (MB)\"] = df_vars[\"Size_MB\"].apply(lambda x: f\"{x:.2f} MB\")",
        "    return df_vars.drop(\"Size_MB\", axis=1).reset_index(drop=True)",
        "",
        "def bulk_delete(vars_list, scope=\"global\", save_deleted=False, return_type=\"dict\"):",
        "    \"\"\"Delete multiple variables from memory (global or local scope).\"\"\"",
        "    deleted_vars = {} if return_type == \"dict\" else []",
        "    variables = globals() if scope == \"global\" else locals()",
        "    for var_name in vars_list:",
        "        if var_name in variables:",
        "            if save_deleted:",
        "                if return_type == \"dict\":",
        "                    deleted_vars[var_name] = variables[var_name]",
        "                else:",
        "                    deleted_vars.append(variables[var_name])",
        "            del variables[var_name]",
        "    gc.collect()",
        "    return deleted_vars if save_deleted else None",
        "",
        "def show_var(MB=1):",
        "    display(show_variables(min_size_mb=MB))",
        "",
        "def delete(vars_list):",
        "    bulk_delete(vars_list)",
        "    display(show_variables(min_size_mb=1))",
        "",
        "show_var()",
        "delete(['variable'])"
    ],
    "description": "Functions for memory management: show_variables displays variables in memory above a size threshold, bulk_delete removes specified variables, show_var displays variables with a custom size threshold, and delete removes variables and shows remaining ones."
},
// ===============================================================================================================================
"check_missing": {
    "prefix": "check_missing",
    "body": [
        "def data_quality(df):",
        "    from pandas.api.types import is_numeric_dtype, is_object_dtype, CategoricalDtype",
        "    missing = df.isnull().sum()",
        "    missing_per = (missing / len(df)) * 100",
        "    missing_df  = pd.DataFrame({",
        "        \"Missing_Count\"      : missing,",
        "        \"Missing_Percentage\" : missing_per,",
        "        \"Column_Type\"        : [\"Numeric\" if is_numeric_dtype(df[c]) else \"Categorical\" if (is_object_dtype(df[c]) or isinstance(df[c].dtype, CategoricalDtype)) else \"Other\" for c in df.columns]",
        "    }).query(\"Missing_Count > 0\").sort_values(\"Missing_Percentage\", ascending=False)",
        "    ",
        "    if not missing_df.empty:",
        "        def color_column_type(val):",
        "            if val == \"Numeric\":",
        "                return \"background-color:#D6EAF8; color:#1B4F72; font-weight:bold;\"",
        "            elif val == \"Categorical\":",
        "                return \"background-color:#FADBD8; color:#641E16; font-weight:bold;\"",
        "            elif val == \"Datetime\":",
        "                return \"background-color:#E8F6F3; color:#0B5345; font-weight:bold;\"",
        "            else:",
        "                return \"background-color:#FDF2E9; color:#7D6608; font-weight:bold;\"",
        "        styled_df = (missing_df.style.applymap(color_column_type, subset=[\"Column_Type\"]).background_gradient(subset=[\"Missing_Percentage\"], cmap=\"Oranges\").format({\"Missing_Count\": \"{:.0f}\", \"Missing_Percentage\": \"{:.2f}%\"}))",
        "        display(styled_df)",
        "        return missing_df",
        "",
        "missing_df = data_quality(df)"
    ],
    "description": "Function to analyze missing data in a Pandas DataFrame, displaying a styled table of columns with missing values, their counts, percentages, and types (Numeric, Categorical, or Other)."
},
// ===============================================================================================================================
"seperate_column_types": {
    "prefix": "seperate_column_types",
    "body": [
        "def seperate_column_types(df, numerics=['float32', 'float64'], show_summary=True):",
        "    \"\"\" Identify numerical and categorical columns in a DataFrame.\"\"\"",
        "    from itertools import zip_longest",
        "    ",
        "    # Get numerical columns (numeric dtypes)",
        "    numerical_columns = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and str(df[c].dtype) in numerics]",
        "    ",
        "    # Get categorical columns (non-numeric dtypes or excluded numeric types)",
        "    categorical_columns = [c for c in df.columns if not pd.api.types.is_numeric_dtype(df[c]) or str(df[c].dtype) not in numerics]",
        "    ",
        "    if show_summary:",
        "        print(f\"Column Type Summary:\")",
        "        print(f\"   Numerical   : {len(numerical_columns)} columns\")",
        "        print(f\"   Categorical : {len(categorical_columns)} columns\")",
        "        print(f\"   Total       : {len(df.columns)} columns\")",
        "        ",
        "        print(\"\\n Numerical Columns                                     | Categorical Columns\")",
        "        print(\"==========================================================================================\")",
        "        for i, (num, cat) in enumerate(zip_longest(numerical_columns, categorical_columns, fillvalue='')):",
        "            print(f\"{i+1:2d}. {num:<50} | {cat:<28}\")",
        "    return numerical_columns, categorical_columns",
        "",
        "numerical_cols, categorical_cols = seperate_column_types(df)"
    ],
    "description": "Function to identify numerical and categorical columns in a Pandas DataFrame, with an optional summary table showing column counts and names."
},
// ================================================================================================================================
"split": {
    "prefix": "split",
    "body": [
        "X = df.drop(target_name, axis=1)",
        "y = df[target_name]",
        "",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size=0.3, random_state=42)",
        "",
        "print(f\"X â†’ Train shape: {X_train.shape}\\t, Test shape: {X_test.shape}\")",
        "print(f\"y â†’ Train shape: {y_train.shape}\\t, Test shape: {y_test.shape}\")"
    ],
    "description": "Code to split a DataFrame into features (X) and target (y), perform a stratified train-test split with a 70-30 ratio, and print the shapes of the resulting training and testing sets."
},
// ================================================================================================================================
"check_imbalace": {
    "prefix": "check_imbalace",
    "body": [
        "# Check if it is balanced or not",
        "print(\"Class Counts:\")",
        "print(df['Class'].value_counts())",
        "print('-------------------')",
        "print(\"Class Percentages:\")",
        "print(df['Class'].value_counts(normalize=True) * 100)",
        "",
        "plt.figure(figsize=(10, 5))",
        "sns.countplot(data=df, x='Class', hue='Class', palette='Set1', legend=False)",
        "plt.title('Class Distribution')",
        "plt.xlabel('Class')",
        "plt.ylabel('Count')",
        "plt.show()",
        "",
        "## if imbalanced is large:",
        "# plt.figure(figsize=(8, 5))",
        "# sns.countplot(data=df, x='Class', hue='Class', palette='Set1')",
        "# plt.yscale('log')  # Logarithmic scale",
        "# plt.title('Class Distribution (Log Scale)')",
        "# plt.ylabel('Count (log scale)')",
        "# plt.xlabel('Class')",
        "# plt.show()"
    ],
    "description": "Code to check class imbalance in a DataFrame by printing class counts and percentages, and plotting a class distribution with an optional logarithmic-scale plot for highly imbalanced data."
}
}